import re
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Input, Embedding, SimpleRNN, Dense
import pickle
from sklearn.model_selection import train_test_split

# -------------------------
# ðŸ›  Part 1: Data Preprocessing & Splitting Dataset
# -------------------------

def read_txt(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        return file.read()

# Read dataset
data = read_txt('dataset.txt')

# âœ… Correct regex pattern
pattern = r'Code:\s*(.*?)\n#\s*(.*?)(?=\nCode:|$)'
matches = re.findall(pattern, data, re.DOTALL)

code_snippets = [code.strip() for code, _ in matches]
comments = [comment.strip() for _, comment in matches]

print("Total Code Snippets:", len(code_snippets))
print("Total Comments:", len(comments))

if len(code_snippets) == 0 or len(comments) == 0:
    raise ValueError("Dataset extraction failed! Check dataset format and regex patterns.")

# Split dataset
train_x, test_x, train_y, test_y = train_test_split(
    code_snippets, comments, test_size=0.2, random_state=42
)

train_y = ['start ' + c + ' end' for c in train_y]
test_y = ['start ' + c + ' end' for c in test_y]

# Tokenization
code_tokenizer = Tokenizer()
code_tokenizer.fit_on_texts(train_x)
code_sequences_train = code_tokenizer.texts_to_sequences(train_x)
code_sequences_test = code_tokenizer.texts_to_sequences(test_x)

comment_tokenizer = Tokenizer()
comment_tokenizer.fit_on_texts(train_y)
comment_sequences_train = comment_tokenizer.texts_to_sequences(train_y)
comment_sequences_test = comment_tokenizer.texts_to_sequences(test_y)

# Padding
max_code_len = max(len(seq) for seq in code_sequences_train)
max_comment_len = max(len(seq) for seq in comment_sequences_train)

code_padded_train = pad_sequences(code_sequences_train, maxlen=max_code_len, padding='post')
code_padded_test = pad_sequences(code_sequences_test, maxlen=max_code_len, padding='post')

comment_padded_train = pad_sequences(comment_sequences_train, maxlen=max_comment_len, padding='post')
comment_padded_test = pad_sequences(comment_sequences_test, maxlen=max_comment_len, padding='post')

# Save tokenizers and lengths
with open("code_tokenizer.pkl", "wb") as f:
    pickle.dump(code_tokenizer, f)
with open("comment_tokenizer.pkl", "wb") as f:
    pickle.dump(comment_tokenizer, f)
with open("max_lengths.pkl", "wb") as f:
    pickle.dump((max_code_len, max_comment_len), f)

# -------------------------
# ðŸ§  Part 2: Build SimpleRNN Seq2Seq Model
# -------------------------

code_input = Input(shape=(max_code_len,))
code_embedding = Embedding(input_dim=len(code_tokenizer.word_index)+1, output_dim=128)(code_input)

encoder_rnn = SimpleRNN(256, return_state=True)
_, state_h = encoder_rnn(code_embedding)

decoder_input = Input(shape=(max_comment_len,))
decoder_embedding = Embedding(input_dim=len(comment_tokenizer.word_index)+1, output_dim=128)(decoder_input)

decoder_rnn = SimpleRNN(256, return_sequences=True)
decoder_output = decoder_rnn(decoder_embedding, initial_state=[state_h])

output_layer = Dense(len(comment_tokenizer.word_index)+1, activation='softmax')(decoder_output)

model = Model([code_input, decoder_input], output_layer)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

# -------------------------
# ðŸš€ Part 3: Train the Model & Evaluate
# -------------------------

decoder_target_data = np.zeros_like(comment_padded_train)
for i in range(len(comment_padded_train)):
    decoder_target_data[i, :-1] = comment_padded_train[i, 1:]

model.fit(
    [code_padded_train, comment_padded_train],
    decoder_target_data,
    epochs=200,  # Tune as needed
    batch_size=32,
    validation_split=0.1
)

model.save("simple_rnn_code_comment.keras")

decoder_test_target_data = np.zeros_like(comment_padded_test)
for i in range(len(comment_padded_test)):
    decoder_test_target_data[i, :-1] = comment_padded_test[i, 1:]

test_loss, test_accuracy = model.evaluate(
    [code_padded_test, comment_padded_test], decoder_test_target_data
)
print(f"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}")

# -------------------------
# ðŸ”¥ Part 4: Inference Function
# -------------------------

def generate_comment(code_text):
    try:
        with open("code_tokenizer.pkl", "rb") as f:
            code_tokenizer = pickle.load(f)
        with open("comment_tokenizer.pkl", "rb") as f:
            comment_tokenizer = pickle.load(f)
        with open("max_lengths.pkl", "rb") as f:
            max_code_len, max_comment_len = pickle.load(f)

        model = load_model("simple_rnn_code_comment.keras")

        code_seq = code_tokenizer.texts_to_sequences([code_text])
        code_padded = pad_sequences(code_seq, maxlen=max_code_len, padding='post')

        target_seq = np.zeros((1, max_comment_len))
        target_seq[0, 0] = comment_tokenizer.word_index.get('start', 1)

        output_words = []

        for i in range(1, max_comment_len):
            output_tokens = model.predict([code_padded, target_seq], verbose=0)
            sampled_token_index = np.argmax(output_tokens[0, i - 1, :])
            sampled_word = comment_tokenizer.index_word.get(sampled_token_index, '')

            if sampled_word == 'end' or sampled_word == '':
                break

            output_words.append(sampled_word)
            target_seq[0, i] = sampled_token_index

        return ' '.join(output_words)
    except Exception as e:
        return f"Error generating comment: {str(e)}"

# -------------------------
# ðŸŽ‰ Part 5: Take User Input & Generate Comments
# -------------------------

while True:
    user_code = input("\nEnter your Python code snippet (or type 'exit' to quit):\n")
    if user_code.lower() == "exit":
        print("Exiting program. Goodbye!")
        break

    generated_comment = generate_comment(user_code)
    print("\nGenerated Comment:", generated_comment)
